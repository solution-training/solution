{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 強化学習\n",
    "---\n",
    "<font size=4.5> __強化学習とは__</font><br>\n",
    "強化学習はAIに__状態、行動、報酬__を与えることで__累積報酬を最大化する<font color=red>手法</font>を学習させる__ことです。<br>\n",
    "<br>\n",
    "AIが手法を学ぶというところがポイントで、得られる報酬を参考に\"将棋で勝つためにはどう駒を動かせばいいか\"、\"車をどう動かせば最短時間で目的地に達成するか\"などの手法を学びます。<br>\n",
    "<br>\n",
    "【用語】<br>\n",
    "__状態__‥将棋の盤面や車の位置、スピードなど強化学習を行うシチュエーションにおける状態のこと。<br>\n",
    "__行動__‥将棋の指し手や車でどう進んでいくかなど強化学習を行うシチュエーションにおける行動のこと。<br>\n",
    "__報酬__‥将棋の駒の取得や損失など行動を行う際に得られる報酬のこと。<br>\n",
    "<br>\n",
    "累積報酬の最大化とは、__目先の報酬を優先するのではなく、最終的に得られた報酬を最大化するように行動する__こと。将棋などでは目先の駒をとることも大事だが、王をとること（＝累積報酬の最大化）が大事なため、最終的に王をとるためどう将棋を打つかをAIに学習させる。また王をとるためにあえて駒をとらせる選択もとれるようになります。<br>\n",
    "<br>\n",
    "<br>\n",
    "【用途】<br>\n",
    "強化学習は主にゲームのCPU、ロボットが行動するための人工知能や自動運転システムなどに使われています。<br>\n",
    "強化学習が得意とするケースは__システムの自動化や機材などの最適化__になります。\n",
    "<br><br>\n",
    "すでに強化学習を用いて、__人間の知能を超えたケース__は数多くあります。<br>\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"picture/図11.png\" width=\"700px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 強化学習の用途\n",
    "\n",
    " -  __自動化__ <br>\n",
    "強化学習によりAIがシステムを自動化、構築してくれるので、人の手でわざわざシステムを構築する必要がない。\n",
    " - __最適化__<br>\n",
    "機材の台数など過不足なく最もいい数を導く。<br>\n",
    "人間のエキスパートが出した解よりも優れた解を発見する可能性がある。<br>\n",
    "\n",
    " - __最大化、最小化__<br>\n",
    "目的地までの最短ルートを発見するなど、目的とする量の最大化、最小化を図る 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 強化学習事例（概要）\n",
    "  - __乗り換えアプリなどの最短時間ルート、最安値ルート__<br>\n",
    "  駅の組み合わせは9000＊9000＝8100万通りあり、すべての最短ルート、最安値ルートを人の手で出すのは困難。<br>\n",
    "  なので、AIにシステムを構築させ、最短ルート、最安値ルートを導く。<br>\n",
    "  - __送電ネットワークの最適化__<br>\n",
    "  消費電力の削減、排出ガスの削減のために、送電ルートをどうすればいいかAIが導く。<br>\n",
    "  - __生産ライン最適化__<br>\n",
    "  故障発生リスクを回避するようAIに生産ラインの制御方法を構築させることで、故障によるコストをカットさせる。<br>\n",
    "  - __在庫管理、商品値段の最適化__<br>\n",
    "  売れない商品の値段を下げすぎないように下げる。\n",
    "  - __ロボットの歩行制御__<br>\n",
    "  ロボットに一つ一つ人間の歩き方をプログラムとして書かせるのは時間も手間もかかる。<br>\n",
    "  強化学習を用いることで、歩き方をプログラムとして書かずとも、自動で歩けるようになる。\n",
    "  - __ロボットの手の制御__<br>\n",
    "  AIに5本指で物を壊さないよう掴む学習をさせたり、できるだけ物を遠いところに運ぶといった手法を学習させることができる。<br>\n",
    "  - __自動運転__<br>\n",
    "  道を外れない、人やモノとぶつからないようAIが運転手法を学習することができる。<br>\n",
    "  - __データセンタの冷却（空間的最適化）__<br>\n",
    "  データセンタの一部のサーバの温度が上昇した際に、どの空調とどの空調をどんな風向きで動かせば、一番効率的にそのサーバ周囲だけを冷却できるのかをAIが制御する。<br>\n",
    "  - __データセンタの冷却（時間的最適化）__<br>\n",
    "  ネットワークの状況やデータセンタ周囲に配置した気温・湿度センサの情報から、一定時間後のサーバの稼働率とデータセンタの周囲の気温を予測し、時間的に無駄な冷却を避けるようAIが制御する。つまり、「このあとサーバの稼働率は下がり、外気の気温も下がってデータセンタ全体の室温も下がるから、このサーバの温度も自然と下がるし、今は急いで冷却しなくていいや」と判断する。<br>\n",
    "  - __囲碁AI__<br>\n",
    "  AlphaGOという囲碁AIは強化学習とディープラーニングを用いて碁の打ち方を学び、プロ棋士9段の方に4勝1敗を収めている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 強化学習の仕組み\n",
    "---\n",
    "基本的に強化学習は、車の物理演算ソフトや対局における対戦相手などの__実行環境が必要__になります。<br>\n",
    "また、強化学習では一度きりのシミュレーションで終了ではなく、同じ行動ルートであっても、何回も何回もシミュレーションしてデータをとっていくことでいい手法を学習します。\n",
    "<br><br>\n",
    "\n",
    "<img src=\"picture/図19.jpg\" width=\"500px\">\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "自動運転の例を用いて強化学習の仕組みを見ていきます。<br>\n",
    "<br>\n",
    "「車を運転し最短時間ルートで目的地に着く」ということをAIにさせたい場合、まずは仮想空間上で車を走行できる環境が必要になります。<br>\n",
    "次に自動運転AIに「速度の出し方」、「ブレーキ」、「ハンドルの動かし方」の行動を与えます。<br>\n",
    "するとAIは「速度の出し方」、「ブレーキ」、「ハンドルの動かし方」をもとに仮想空間上で車を動かしていき、どうすれば最短ルートで着くことができるか__試行錯誤__していきます。<br>\n",
    "<br>\n",
    "幾度となく試行錯誤するうちに、「このルートは近いように見えて坂道が多く時間がかかる」とか、「このルートは道が整備されてて走りやすい」とかがわかるようになり、最短時間ルートを総合的に導くことができます。\n",
    "<br><br>\n",
    "これが強化学習における自動化の仕組みになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また強化学習とは、【状態】と【行動】と【報酬】をAIに与えることで、累積報酬の最大化を目指すことだったのでこの例にあてはめると、<br>\n",
    "__状態__‥車がどの向きにいるか、どこにいるか、スピードはどれくらいか<br>\n",
    "__行動__‥速度の出し方、ブレーキ、ハンドルの動かし方<br>\n",
    "__累積報酬の最大化__‥最短時間で着けばつくほど、AIに報酬を与えるよう設定<br>\n",
    "になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 強化学習まとめ\n",
    "---\n",
    "__<メリット>__<br>\n",
    "人の学習速度を超えれるため、今行っている手法よりいい手法が導ける。<br>\n",
    "自分でデータを取りに行きながら学習するため、わざわざプログラムする必要がなく自動化にたける。<br>\n",
    "全ての組み合わせを試すことなくより良い手法を構築できる。<br>\n",
    "<br>\n",
    "\n",
    "__<デメリット>__<br>\n",
    "自分でデータを取得して評価、更新をかけるため学習時間が長い。<br>\n",
    "組み合わせ爆発が起こるケースは、学習結果が膨大になる。<br>\n",
    "車の走行など、仮想空間で検証するための環境が必要なことが多い。<br>\n",
    "※学習してないことはAIにもできないため、AIが何を学習しているか把握する必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※特にこのデメリットは大きい。自動運転であれば、一通りAIに学習させた後自動運転をさせた場合、このデメリットを無視すると事故が起こる可能性が高くなる。例えば路面凍結したシミュレーションでAIに学習させていなかった場合、AIは路面凍結した際の走行手法がわからず、事故につながる。こういったAIが何を学習しているかを把握するのもデータサイエンティストの仕事になる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 強化学習実行\n",
    "---\n",
    "__シチュエーション__<br>\n",
    "AIが迷路をたどり最短ルートで地雷を踏まず宝箱まで行きつくようにしたい。<br>\n",
    "AIはどこに地雷、宝箱があるかわからないとする。<br>\n",
    "AIはどこに宝箱があるかわからないので、実際に何度も何度も試行錯誤しながら迷路を進んでいく。<br>\n",
    "進んでいく中で宝箱を得た場合、「このルートは良さそうだ」と判断し、「もっといいルートはないのか」を考えながら初めからやり直し、試行錯誤していくことで最短ルートで地雷を踏まず宝箱まで行きつけるようになる。\n",
    "\n",
    "<img src=\"picture/図23.png\" width=\"500px\">\n",
    "参考コード：https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "迷路の世界のルールは以下の通りです。\n",
    " - AIはあるグリッドの中にいます\n",
    " - AIは壁(灰色)を越えられません\n",
    " - AIは自分が思った通りに動けるとは限りません <br>\n",
    "80%の確率で正しく動けますが、20%の確率で動こうと思っていた方向とは別の方向に動く\n",
    " - 動こうと思った場所に壁があった場合は、その場にとどまります\n",
    " - ゴール(宝箱)やトラップ(爆発)にたどり着くとゲーム終了し、スタート位置に戻ります"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▼AIはどうすればより良い高率で宝箱まで達成できるか試行錯誤し、いい行動ルートを模索する。（どれも最適なルートではない音に注意。）\n",
    "<img src=\"picture/図24.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それではAIがどう動けば地雷を踏まず宝箱まで行きつくようなルートを探し出すか見ていきたいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from utils import vector_add, orientations, turn_right, turn_left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▼迷路の世界を表現するためのクラスです。仮想環境を作っています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "\n",
    "    def __init__(self, init, actlist, terminals, gamma=.9):\n",
    "        self.init = init\n",
    "        self.actlist = actlist\n",
    "        self.terminals = terminals\n",
    "        if not (0 <= gamma < 1):\n",
    "            raise ValueError(\"An MDP must have 0 <= gamma < 1\")\n",
    "        self.gamma = gamma\n",
    "        self.states = set()\n",
    "        self.reward = {}\n",
    "\n",
    "    def R(self, state):\n",
    "        return self.reward[state]\n",
    "\n",
    "    def T(self, state, action):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def actions(self, state):\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.actlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▼ここで試行錯誤するためのプログラムを作っています。<br>\n",
    "　具体的にはAIに行動するための選択肢と報酬を与えています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridMDP(MDP):\n",
    "\n",
    "    def __init__(self, grid, terminals, init=(0, 0), gamma=.9):\n",
    "        grid.reverse()  # because we want row 0 on bottom, not on top                                                                                                  \n",
    "        MDP.__init__(self, init, actlist=orientations,\n",
    "                     terminals=terminals, gamma=gamma)\n",
    "        self.grid = grid\n",
    "        self.rows = len(grid)\n",
    "        self.cols = len(grid[0])\n",
    "        for x in range(self.cols):\n",
    "            for y in range(self.rows):\n",
    "                self.reward[x, y] = grid[y][x]\n",
    "                if grid[y][x] is not None:\n",
    "                    self.states.add((x, y))\n",
    "\n",
    "    def T(self, state, action):\n",
    "        if action is None:\n",
    "            return [(0.0, state)]\n",
    "        else:\n",
    "            return [(0.8, self.go(state, action)),\n",
    "                    (0.1, self.go(state, turn_right(action))),\n",
    "                    (0.1, self.go(state, turn_left(action)))]\n",
    "\n",
    "    def go(self, state, direction):\n",
    "        state1 = vector_add(state, direction)\n",
    "        return state1 if state1 in self.states else state\n",
    "\n",
    "    def to_grid(self, mapping):\n",
    "        return list(reversed([[mapping.get((x, y), None)\n",
    "                               for x in range(self.cols)]\n",
    "                              for y in range(self.rows)]))\n",
    "\n",
    "    def to_arrows(self, policy):\n",
    "        chars = {(1, 0): '>', (0, 1): '^', (-1, 0): '<', (0, -1): 'v', None: '.'}\n",
    "        return self.to_grid({s: chars[a] for (s, a) in policy.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▼ここでは今までの行動ルートを記憶し、その中で最もいいルートを導くプログラムです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def value_iteration(mdp, epsilon=0.001):\n",
    "    U1 = {s: 0 for s in mdp.states}\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    while True:\n",
    "        U = U1.copy()\n",
    "        delta = 0\n",
    "        for s in mdp.states:\n",
    "            U1[s] = R(s) + gamma * max([sum([p * U[s1] for (p, s1) in T(s, a)])\n",
    "                                        for a in mdp.actions(s)])\n",
    "            delta = max(delta, abs(U1[s] - U[s]))\n",
    "        if delta < epsilon * (1 - gamma) / gamma:\n",
    "            return U\n",
    "\n",
    "\n",
    "def best_policy(mdp, U):\n",
    "    pi = {}\n",
    "    for s in mdp.states:\n",
    "        pi[s] = argmax(mdp.actions(s), key=lambda a: expected_utility(a, s, U, mdp))\n",
    "    return pi\n",
    "\n",
    "\n",
    "def expected_utility(a, s, U, mdp):\n",
    "    return sum([p * U[s1] for (p, s1) in mdp.T(s, a)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▼迷路のパラメータ設定です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_decision_environment = GridMDP([[-0.04, -0.04, -0.04, +1],\n",
    "                                           [-0.04, None,  -0.04, -1],\n",
    "                                           [-0.04, -0.04, -0.04, -0.04]],\n",
    "                                          terminals=[(3, 2), (3, 1)])\n",
    "\n",
    "pi = best_policy(sequential_decision_environment, value_iteration(sequential_decision_environment, .01))\n",
    "\n",
    "print_table(sequential_decision_environment.to_arrows(pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上が地雷を踏まず、宝箱を得ることができる最適ルートになります。いい感じに目的地までたどり着くことができました。<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
